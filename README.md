# Mini-Infer

ä¸€ä¸ªè½»é‡æå¤§ã€é«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶ï¼Œæ¶æ„è®¾è®¡çµæ„Ÿæºè‡ª TensorRT å’Œ PyTorchã€‚æˆ‘ä»¬è¿½æ±‚æè‡´çš„ **Zero-Copy** å’Œ **Static Memory Planning**ã€‚

## é¡¹ç›®ç‰¹æ€§

- ğŸš„ **é«˜æ€§èƒ½ (High Performance)**:
    - **é™æ€å†…å­˜è§„åˆ’**: é‡‡ç”¨ Linear Scan ç®—æ³•ï¼Œå°†æ‰€æœ‰ä¸­é—´å¼ é‡å‹ç¼©åˆ°ä¸€å—è¿ç»­å†…å­˜ä¸­ï¼Œæå¤§é™ä½ç¢ç‰‡å’Œåˆ†é…å¼€é”€ã€‚
    - **é›¶æ‹·è´**: Tensor View è®¾è®¡ï¼Œæ”¯æŒåˆ‡ç‰‡å’Œ Reshape è€Œä¸äº§ç”Ÿæ•°æ®æ‹·è´ã€‚
    - **æ— é” Allocator**: é’ˆå¯¹é«˜æ€§èƒ½åœºæ™¯ä¼˜åŒ–çš„å†…å­˜åˆ†é…å™¨ã€‚
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡ (Modular Architecture)**:
    - **Core**: åŸºç¡€æ•°æ®ç»“æ„ (Tensor/Storage)ã€‚
    - **Runtime**: æ¨ç†å¼•æ“ (InferencePlan/ExecutionContext)ï¼Œæ”¯æŒå¹¶å‘æ¨ç†ã€‚
    - **Backends**: å¼‚æ„è®¾å¤‡ç®¡ç† (DeviceContext/Registry)ï¼Œæ”¯æŒ CPU/CUDA çƒ­æ’æ‹”ã€‚
- ğŸ”Œ **ONNX æ”¯æŒ**:
    - å†…ç½® ONNX è§£æå™¨ï¼Œæ”¯æŒå°† ONNX æ¨¡å‹ç›´æ¥å¯¼å…¥ä¸ºè®¡ç®—å›¾ã€‚
    - **Port-Based Graph**: æ”¯æŒå¤šè¾“å…¥å¤šè¾“å‡º (MIMO) çš„å¤æ‚æ‹“æ‰‘ç»“æ„ã€‚

## é¡¹ç›®ç»“æ„

```
Mini-Infer/
â”œâ”€â”€ include/mini_infer/
â”‚   â”œâ”€â”€ core/          # æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ˆTensor, Storage, Allocatorï¼‰
â”‚   â”œâ”€â”€ backends/      # æ‰§è¡Œç¯å¢ƒï¼ˆDeviceContextï¼‰
â”‚   â”œâ”€â”€ kernels/       # ç®—å­æ³¨å†Œè¡¨ä¸å†…æ ¸ï¼ˆRegistry, Dispatcherï¼‰
â”‚   â”œâ”€â”€ graph/         # è®¡DAG å›¾ç»“æ„ï¼ˆNode, Edge, Portï¼‰
â”‚   â”œâ”€â”€ runtime/       # æ¨ç†å¼•æ“ï¼ˆPlan, Context, MemoryPlannerï¼‰
â”‚   â””â”€â”€ importers/     # æ¨¡å‹å¯¼å…¥ï¼ˆOnnxParserï¼‰
â”œâ”€â”€ src/               # æºä»£ç å®ç°
â””â”€â”€ examples/          # ç¤ºä¾‹ä»£ç 
```

## å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… Conan åŒ…ç®¡ç†å™¨
pip install conan

# æ£€æµ‹é»˜è®¤é…ç½®
conan profile detect --force
```

### 2. æ„å»ºé¡¹ç›®

ä½¿ç”¨ **Conan** è‡ªåŠ¨ç®¡ç†ä¾èµ–å¹¶æ„å»ºï¼š

```bash
# Windows (PowerShell)
# æ­¥éª¤ 1: å®‰è£…ä¾èµ–å¹¶ç”Ÿæˆ CMake é¢„è®¾
conan install . --output-folder=build --build=missing -s build_type=Release

# æ­¥éª¤ 2: é…ç½® CMake
cmake --preset conan-release

# æ­¥éª¤ 3: ç¼–è¯‘
cmake --build --preset conan-release

# Linux/macOS (Bash)
# æ­¥éª¤ 1: å®‰è£…ä¾èµ–å¹¶ç”Ÿæˆ CMake é¢„è®¾
conan install . --output-folder=build --build=missing -s build_type=Release

# æ­¥éª¤ 2: é…ç½® CMake
cmake --preset conan-release

# æ­¥éª¤ 3: ç¼–è¯‘
cmake --build --preset conan-release
```

### 3. è¿è¡Œç¤ºä¾‹ (Run Example)

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª LeNet-5 çš„å®Œæ•´ç¤ºä¾‹ï¼š

```bash
# Windows
.\build\Release\bin\onnx_parser_example.exe .\models\python\lenet5\models\lenet5.onnx

# Linux/macOS
./build/Release/bin/onnx_parser_example ./models/python/lenet5/models/lenet5.onnx
```

### 4. C++ API ç¤ºä¾‹

```cpp
#include "mini_infer/runtime/engine.h"
#include "mini_infer/importers/onnx_parser.h"

using namespace mini_infer;

int main() {
    // 1. è§£æ ONNX
    importers::OnnxParser parser;
    auto graph = parser.parse_from_file("model.onnx");

    // 2. é…ç½®å¼•æ“
    runtime::EngineConfig config;
    config.enable_memory_planning = true; // å¼€å¯é™æ€å†…å­˜è§„åˆ’
    runtime::Engine engine(config);

    // 3. æ„å»º Plan (Optimization + Memory Planning)
    engine.build(graph);

    // 4. åˆ›å»º Context å¹¶æ‰§è¡Œ
    auto ctx = engine.create_context();
    
    // å‡†å¤‡æ•°æ®
    auto input_tensor = core::Tensor::create({1, 3, 224, 224});
    // ... fill data ...
    
    ctx->set_input("input", input_tensor);
    engine.execute(ctx.get()); // é›¶æ‹·è´æ‰§è¡Œ

    // è·å–ç»“æœ
    auto output = ctx->get_output("output");
}
```

### 5. æ„å»ºé€‰é¡¹

Conan æä¾›äº†çµæ´»çš„æ„å»ºé€‰é¡¹ï¼š

```bash
# å¯ç”¨ CUDA æ”¯æŒ
conan install . --output-folder=build --build=missing \
  -o enable_cuda=True \
  -o cuda_toolkit_root="C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3"

# ç¦ç”¨ ONNX æ”¯æŒï¼ˆå‡å°äºŒè¿›åˆ¶å¤§å°ï¼‰
conan install . --output-folder=build --build=missing \
  -o enable_onnx=False

# ç¦ç”¨æ—¥å¿—ï¼ˆç”Ÿäº§ç¯å¢ƒä¼˜åŒ–ï¼‰
conan install . --output-folder=build --build=missing \
  -o enable_logging=False
```

è¯¦ç»†çš„æ„å»ºé€‰é¡¹è¯·å‚è€ƒ [Conan æ„å»ºæŒ‡å—](docs/CONAN_BUILD_GUIDE.md)ã€‚

## ğŸ“š æ–‡æ¡£

- **[å¿«é€Ÿå¼€å§‹](QUICK_START.md)** - å¿«é€Ÿä¸Šæ‰‹æŒ‡å—
- **[Conan æ„å»ºæŒ‡å—](docs/CONAN_BUILD_GUIDE.md)** - è¯¦ç»†çš„ Conan ä½¿ç”¨è¯´æ˜
- **[CUDA é…ç½®æŒ‡å—](docs/CUDA_CONAN_SETUP.md)** - CUDA åç«¯é…ç½®
- **[æ¶æ„è®¾è®¡](docs/ARCHITECTURE.md)** - è¯¦ç»†çš„æ¶æ„è®¾è®¡æ–‡æ¡£
- **[API æ–‡æ¡£](docs/API.md)** - API å‚è€ƒæ‰‹å†Œ
- **[å…¥é—¨æ•™ç¨‹](docs/GETTING_STARTED.md)** - å®Œæ•´çš„å…¥é—¨æ•™ç¨‹

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼æˆ‘ä»¬æ­£åœ¨ç§¯æå¯»æ‰¾ä»¥ä¸‹è´¡çŒ®ï¼š
- [ ] SIMD ä¼˜åŒ– (AVX2/NEON) for CPU Kernels
- [ ] CUDA Kernels å®ç°
- [ ] æ›´å¤š ONNX ç®—å­æ”¯æŒ

## ğŸ“„ è®¸å¯è¯

MIT License
