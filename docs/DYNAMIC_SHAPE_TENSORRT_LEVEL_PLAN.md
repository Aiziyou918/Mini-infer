# åŠ¨æ€ Shape æ”¯æŒæå‡è®¡åˆ’ (TensorRT çº§åˆ«)

## ğŸ¯ ç›®æ ‡

å°† Mini-Infer çš„åŠ¨æ€ Shape æ”¯æŒæå‡åˆ° **TensorRT çº§åˆ«**ï¼Œå®ç°ï¼š
- âœ… ä»»æ„ç»´åº¦åŠ¨æ€
- âœ… Optimization Profile
- âœ… è¿è¡Œæ—¶é‡æ¨æ–­
- âœ… åŠ¨æ€å†…å­˜ç®¡ç†
- âœ… æ€§èƒ½ä¼˜åŒ–

---

## ğŸ“Š å½“å‰çŠ¶æ€ vs TensorRT

| åŠŸèƒ½ | Mini-Infer (å½“å‰) | TensorRT | ç›®æ ‡ |
|-----|------------------|----------|------|
| åŠ¨æ€ Batch | âœ… åŸºç¡€æ”¯æŒ | âœ… å®Œæ•´æ”¯æŒ | æå‡åˆ°å®Œæ•´ |
| åŠ¨æ€ H/W/C | âš ï¸ æœªæµ‹è¯• | âœ… å®Œæ•´æ”¯æŒ | æ–°å¢æ”¯æŒ |
| Optimization Profile | âŒ æ—  | âœ… Min/Opt/Max | æ–°å¢ |
| è¿è¡Œæ—¶é‡æ¨æ–­ | âŒ æ—  | âœ… è‡ªåŠ¨ | æ–°å¢ |
| åŠ¨æ€å†…å­˜æ±  | âš ï¸ æœ‰é™ | âœ… å®Œæ•´ | å¢å¼º |
| Shape ç¼“å­˜ | âŒ æ—  | âœ… æœ‰ | æ–°å¢ |
| æ€§èƒ½åˆ†æ | âš ï¸ åŸºç¡€ | âœ… è¯¦ç»† | å¢å¼º |

---

## ğŸ—ºï¸ å®æ–½è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒåŸºç¡€è®¾æ–½ (2-3 å‘¨)

#### 1.1 Optimization Profile ç³»ç»Ÿ

**ç›®æ ‡**: æ”¯æŒå®šä¹‰è¾“å…¥å½¢çŠ¶çš„ Min/Opt/Max èŒƒå›´

**TensorRT API å‚è€ƒ**:
```cpp
// TensorRT
IOptimizationProfile* profile = builder->createOptimizationProfile();
profile->setDimensions("input", OptProfileSelector::kMIN, Dims4{1, 3, 224, 224});
profile->setDimensions("input", OptProfileSelector::kOPT, Dims4{4, 3, 224, 224});
profile->setDimensions("input", OptProfileSelector::kMAX, Dims4{8, 3, 512, 512});
config->addOptimizationProfile(profile);
```

**Mini-Infer å®ç°**:

```cpp
// include/mini_infer/runtime/optimization_profile.h
namespace mini_infer {
namespace runtime {

/**
 * @brief Shape range for optimization profile
 */
struct ShapeRange {
    core::Shape min;   // Minimum shape
    core::Shape opt;   // Optimal shape (for optimization)
    core::Shape max;   // Maximum shape
    
    ShapeRange() = default;
    ShapeRange(const core::Shape& min_, const core::Shape& opt_, const core::Shape& max_)
        : min(min_), opt(opt_), max(max_) {}
    
    bool is_valid() const;
    bool contains(const core::Shape& shape) const;
};

/**
 * @brief Optimization Profile (TensorRT-style)
 * 
 * Defines the range of valid input shapes and helps the engine
 * optimize for specific shape ranges.
 */
class OptimizationProfile {
public:
    OptimizationProfile() = default;
    
    /**
     * @brief Set shape range for an input
     * 
     * @param input_name Name of the input tensor
     * @param min Minimum shape
     * @param opt Optimal shape (for kernel selection and optimization)
     * @param max Maximum shape
     * @return Status
     */
    core::Status set_shape_range(
        const std::string& input_name,
        const core::Shape& min,
        const core::Shape& opt,
        const core::Shape& max
    );
    
    /**
     * @brief Get shape range for an input
     */
    const ShapeRange* get_shape_range(const std::string& input_name) const;
    
    /**
     * @brief Check if a set of input shapes is valid for this profile
     */
    bool is_valid_for(const std::map<std::string, core::Shape>& shapes) const;
    
    /**
     * @brief Get all input names in this profile
     */
    std::vector<std::string> get_input_names() const;
    
private:
    std::map<std::string, ShapeRange> shape_ranges_;
};

} // namespace runtime
} // namespace mini_infer
```

**å®ç°æ–‡ä»¶**:
```cpp
// src/runtime/optimization_profile.cpp
#include "mini_infer/runtime/optimization_profile.h"

namespace mini_infer {
namespace runtime {

bool ShapeRange::is_valid() const {
    // All shapes must have same ndim
    if (min.ndim() != opt.ndim() || opt.ndim() != max.ndim()) {
        return false;
    }
    
    // For each dimension: min <= opt <= max
    for (size_t i = 0; i < min.ndim(); ++i) {
        int64_t min_dim = min[i];
        int64_t opt_dim = opt[i];
        int64_t max_dim = max[i];
        
        // Skip dynamic dimensions
        if (min_dim < 0 || opt_dim < 0 || max_dim < 0) continue;
        
        if (!(min_dim <= opt_dim && opt_dim <= max_dim)) {
            return false;
        }
    }
    
    return true;
}

bool ShapeRange::contains(const core::Shape& shape) const {
    if (shape.ndim() != min.ndim()) {
        return false;
    }
    
    for (size_t i = 0; i < shape.ndim(); ++i) {
        int64_t dim = shape[i];
        int64_t min_dim = min[i];
        int64_t max_dim = max[i];
        
        // Skip dynamic dimensions in range
        if (min_dim < 0 || max_dim < 0) continue;
        
        if (dim < min_dim || dim > max_dim) {
            return false;
        }
    }
    
    return true;
}

core::Status OptimizationProfile::set_shape_range(
    const std::string& input_name,
    const core::Shape& min,
    const core::Shape& opt,
    const core::Shape& max
) {
    ShapeRange range(min, opt, max);
    
    if (!range.is_valid()) {
        return core::Status::ERROR_INVALID_ARGUMENT;
    }
    
    shape_ranges_[input_name] = range;
    return core::Status::SUCCESS;
}

const ShapeRange* OptimizationProfile::get_shape_range(const std::string& input_name) const {
    auto it = shape_ranges_.find(input_name);
    if (it == shape_ranges_.end()) {
        return nullptr;
    }
    return &it->second;
}

bool OptimizationProfile::is_valid_for(const std::map<std::string, core::Shape>& shapes) const {
    for (const auto& [name, range] : shape_ranges_) {
        auto it = shapes.find(name);
        if (it == shapes.end()) {
            return false;  // Missing input
        }
        
        if (!range.contains(it->second)) {
            return false;  // Shape out of range
        }
    }
    
    return true;
}

std::vector<std::string> OptimizationProfile::get_input_names() const {
    std::vector<std::string> names;
    for (const auto& [name, _] : shape_ranges_) {
        names.push_back(name);
    }
    return names;
}

} // namespace runtime
} // namespace mini_infer
```

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»º `OptimizationProfile` ç±»
- [ ] å®ç° `ShapeRange` éªŒè¯é€»è¾‘
- [ ] æ·»åŠ åˆ° `EngineConfig`
- [ ] å•å…ƒæµ‹è¯•
- [ ] æ–‡æ¡£

**é¢„è®¡æ—¶é—´**: 3-4 å¤©

---

#### 1.2 è¿è¡Œæ—¶ Shape æ¨æ–­å¼•æ“

**ç›®æ ‡**: æ”¯æŒåœ¨ `forward()` æ—¶æ ¹æ®å®é™…è¾“å…¥é‡æ–°æ¨æ–­å½¢çŠ¶

**TensorRT è¡Œä¸º**:
- æ£€æµ‹è¾“å…¥å½¢çŠ¶å˜åŒ–
- è‡ªåŠ¨é‡æ–°æ¨æ–­æ‰€æœ‰ä¸­é—´ tensor å½¢çŠ¶
- æ›´æ–°å†…å­˜åˆ†é…

**Mini-Infer å®ç°**:

```cpp
// include/mini_infer/runtime/shape_inference_engine.h
namespace mini_infer {
namespace runtime {

/**
 * @brief Shape inference context for runtime
 * 
 * Caches shape inference results for different input shapes
 */
class ShapeInferenceEngine {
public:
    ShapeInferenceEngine() = default;
    
    /**
     * @brief Infer shapes for entire graph given input shapes
     * 
     * @param graph The computation graph
     * @param input_shapes Map of input name to shape
     * @param output_shapes Output: inferred shapes for all tensors
     * @return Status
     */
    core::Status infer_shapes(
        std::shared_ptr<graph::Graph> graph,
        const std::map<std::string, core::Shape>& input_shapes,
        std::map<std::string, core::Shape>& output_shapes
    );
    
    /**
     * @brief Check if shapes have been cached for given inputs
     */
    bool has_cached_shapes(const std::map<std::string, core::Shape>& input_shapes) const;
    
    /**
     * @brief Get cached shapes
     */
    const std::map<std::string, core::Shape>* get_cached_shapes(
        const std::map<std::string, core::Shape>& input_shapes
    ) const;
    
    /**
     * @brief Clear shape cache
     */
    void clear_cache();
    
    /**
     * @brief Get cache statistics
     */
    struct CacheStats {
        size_t total_inferences = 0;
        size_t cache_hits = 0;
        size_t cache_misses = 0;
        
        double hit_rate() const {
            return total_inferences > 0 
                ? static_cast<double>(cache_hits) / total_inferences 
                : 0.0;
        }
    };
    
    CacheStats get_cache_stats() const { return stats_; }
    
private:
    // Cache key: hash of input shapes
    struct ShapeCacheKey {
        std::map<std::string, core::Shape> shapes;
        
        bool operator==(const ShapeCacheKey& other) const;
        size_t hash() const;
    };
    
    struct ShapeCacheKeyHash {
        size_t operator()(const ShapeCacheKey& key) const {
            return key.hash();
        }
    };
    
    // Cache: input shapes -> all tensor shapes
    std::unordered_map<
        ShapeCacheKey, 
        std::map<std::string, core::Shape>,
        ShapeCacheKeyHash
    > cache_;
    
    CacheStats stats_;
};

} // namespace runtime
} // namespace mini_infer
```

**å®ç°è¦ç‚¹**:
```cpp
core::Status ShapeInferenceEngine::infer_shapes(
    std::shared_ptr<graph::Graph> graph,
    const std::map<std::string, core::Shape>& input_shapes,
    std::map<std::string, core::Shape>& output_shapes
) {
    stats_.total_inferences++;
    
    // Check cache first
    ShapeCacheKey key{input_shapes};
    auto it = cache_.find(key);
    if (it != cache_.end()) {
        stats_.cache_hits++;
        output_shapes = it->second;
        return core::Status::SUCCESS;
    }
    
    stats_.cache_misses++;
    
    // Perform shape inference (topological order)
    auto sorted_nodes = graph->topological_sort();
    
    // Set input shapes
    for (const auto& [name, shape] : input_shapes) {
        output_shapes[name] = shape;
    }
    
    // Infer each node
    for (auto& node : sorted_nodes) {
        if (!node || !node->get_operator()) continue;
        
        // Collect input shapes
        std::vector<core::Shape> node_input_shapes;
        for (const auto& input_node : node->inputs()) {
            if (input_node) {
                auto it = output_shapes.find(input_node->name());
                if (it != output_shapes.end()) {
                    node_input_shapes.push_back(it->second);
                }
            }
        }
        
        // Add weight shapes
        for (const auto& tensor : node->input_tensors()) {
            if (tensor) {
                node_input_shapes.push_back(tensor->shape());
            }
        }
        
        // Infer output shapes
        std::vector<core::Shape> node_output_shapes;
        auto status = node->get_operator()->infer_shape(
            node_input_shapes, 
            node_output_shapes
        );
        
        if (status != core::Status::SUCCESS) {
            return status;
        }
        
        // Store output shapes
        if (!node_output_shapes.empty()) {
            output_shapes[node->name()] = node_output_shapes[0];
        }
    }
    
    // Cache results
    cache_[key] = output_shapes;
    
    return core::Status::SUCCESS;
}
```

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»º `ShapeInferenceEngine` ç±»
- [ ] å®ç°å½¢çŠ¶æ¨æ–­é€»è¾‘
- [ ] å®ç°å½¢çŠ¶ç¼“å­˜ï¼ˆhash keyï¼‰
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] å•å…ƒæµ‹è¯•

**é¢„è®¡æ—¶é—´**: 5-6 å¤©

---

#### 1.3 åŠ¨æ€å†…å­˜ç®¡ç†å™¨

**ç›®æ ‡**: æ”¯æŒæ ¹æ®å®é™…å½¢çŠ¶åŠ¨æ€åˆ†é…å’Œé‡ç”¨å†…å­˜

**TensorRT è¡Œä¸º**:
- æ ¹æ® Optimization Profile é¢„åˆ†é…å†…å­˜æ± 
- è¿è¡Œæ—¶æ ¹æ®å®é™…å½¢çŠ¶è°ƒæ•´
- æœ€å°åŒ–é‡æ–°åˆ†é…

**Mini-Infer å®ç°**:

```cpp
// include/mini_infer/runtime/dynamic_memory_manager.h
namespace mini_infer {
namespace runtime {

/**
 * @brief Dynamic memory manager (TensorRT-style)
 * 
 * Manages memory allocation for tensors with dynamic shapes
 */
class DynamicMemoryManager {
public:
    DynamicMemoryManager() = default;
    
    /**
     * @brief Prepare memory pools based on optimization profile
     * 
     * Pre-allocate pools based on max shapes in profile
     * 
     * @param profile Optimization profile with shape ranges
     * @param plan Static memory plan (from build time)
     * @return Status
     */
    core::Status prepare(
        const OptimizationProfile& profile,
        const MemoryPlan& plan
    );
    
    /**
     * @brief Allocate memory for actual shapes
     * 
     * Reuse pre-allocated pools if possible, otherwise allocate new
     * 
     * @param tensor_shapes Actual tensor shapes
     * @param allocations Output: allocated memory for each tensor
     * @return Status
     */
    core::Status allocate_for_shapes(
        const std::map<std::string, core::Shape>& tensor_shapes,
        std::map<std::string, std::shared_ptr<void>>& allocations
    );
    
    /**
     * @brief Get memory statistics
     */
    struct MemoryStats {
        size_t pool_capacity = 0;      // Total pool capacity
        size_t pool_used = 0;          // Currently used
        size_t peak_usage = 0;         // Peak usage
        size_t reallocations = 0;      // Number of reallocations
        
        double utilization() const {
            return pool_capacity > 0 
                ? static_cast<double>(pool_used) / pool_capacity 
                : 0.0;
        }
    };
    
    MemoryStats get_stats() const { return stats_; }
    
    /**
     * @brief Reset and clear all allocations
     */
    void reset();
    
private:
    struct MemoryPool {
        std::string name;
        size_t capacity;
        std::shared_ptr<void> data;
        std::vector<std::string> tensor_names;
    };
    
    std::vector<MemoryPool> pools_;
    MemoryStats stats_;
};

} // namespace runtime
} // namespace mini_infer
```

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»º `DynamicMemoryManager` ç±»
- [ ] å®ç°åŸºäº Profile çš„é¢„åˆ†é…
- [ ] å®ç°è¿è¡Œæ—¶åˆ†é…ç­–ç•¥
- [ ] å†…å­˜æ± å¤ç”¨é€»è¾‘
- [ ] æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

**é¢„è®¡æ—¶é—´**: 4-5 å¤©

---

### Phase 2: Engine é›†æˆ (1-2 å‘¨)

#### 2.1 æ‰©å±• EngineConfig

```cpp
// include/mini_infer/runtime/engine.h
struct EngineConfig {
    // ... existing fields ...
    
    // Dynamic shape support
    bool enable_dynamic_shapes = false;
    
    // Optimization profiles (can have multiple)
    std::vector<std::shared_ptr<OptimizationProfile>> optimization_profiles;
    
    // Active profile index
    int active_profile_index = 0;
    
    // Shape cache settings
    bool enable_shape_cache = true;
    size_t max_shape_cache_size = 100;
    
    // Memory management
    bool enable_dynamic_memory = true;
    size_t memory_pool_growth_factor = 2;  // 2x growth when resizing
};
```

#### 2.2 æ‰©å±• Engine ç±»

```cpp
class Engine {
public:
    // ... existing methods ...
    
    /**
     * @brief Add optimization profile
     * 
     * Must be called before build()
     */
    core::Status add_optimization_profile(
        std::shared_ptr<OptimizationProfile> profile
    );
    
    /**
     * @brief Set active optimization profile
     * 
     * @param index Profile index (0-based)
     */
    core::Status set_active_profile(int index);
    
    /**
     * @brief Get current active profile
     */
    const OptimizationProfile* get_active_profile() const;
    
    /**
     * @brief Get shape inference statistics
     */
    ShapeInferenceEngine::CacheStats get_shape_inference_stats() const;
    
    /**
     * @brief Get dynamic memory statistics
     */
    DynamicMemoryManager::MemoryStats get_memory_stats() const;
    
private:
    /**
     * @brief Prepare for dynamic shapes (called in build())
     */
    core::Status prepare_dynamic_shapes();
    
    /**
     * @brief Handle shape change at runtime (called in forward())
     */
    core::Status handle_shape_change(
        const std::map<std::string, std::shared_ptr<core::Tensor>>& inputs
    );
    
    // New members
    std::unique_ptr<ShapeInferenceEngine> shape_engine_;
    std::unique_ptr<DynamicMemoryManager> memory_manager_;
    std::map<std::string, core::Shape> last_input_shapes_;
};
```

#### 2.3 ä¿®æ”¹ build() æµç¨‹

```cpp
core::Status Engine::build(std::shared_ptr<graph::Graph> graph) {
    MI_LOG_INFO("[Engine] Building Engine (dynamic shape support)");
    
    // Step 1: Graph optimization
    optimize_graph();
    
    // Step 2: Topological sort
    topological_sort();
    
    // Step 3: Shape inference (using optimal shapes from profile)
    if (config_.enable_dynamic_shapes && !config_.optimization_profiles.empty()) {
        const auto& profile = config_.optimization_profiles[config_.active_profile_index];
        
        // Use optimal shapes for build-time inference
        std::map<std::string, core::Shape> opt_shapes;
        for (const auto& input_name : graph_->inputs()) {
            const auto* range = profile->get_shape_range(input_name);
            if (range) {
                opt_shapes[input_name] = range->opt;
            }
        }
        
        infer_shapes_with(opt_shapes);
    } else {
        infer_shapes();  // Traditional static inference
    }
    
    // Step 4: Memory planning
    plan_memory();
    
    // Step 5: Prepare dynamic shape support
    if (config_.enable_dynamic_shapes) {
        prepare_dynamic_shapes();
    }
    
    // Step 6: Allocate tensors
    allocate_tensors();
    
    MI_LOG_INFO("[Engine] Engine built successfully");
    return core::Status::SUCCESS;
}
```

#### 2.4 ä¿®æ”¹ forward() æµç¨‹

```cpp
TensorMap Engine::forward(const TensorMap& inputs) override {
    // Step 1: Check if input shapes changed
    bool shape_changed = false;
    std::map<std::string, core::Shape> current_shapes;
    
    for (const auto& [name, tensor] : inputs) {
        current_shapes[name] = tensor->shape();
        
        auto it = last_input_shapes_.find(name);
        if (it == last_input_shapes_.end() || 
            it->second.to_string() != tensor->shape().to_string()) {
            shape_changed = true;
        }
    }
    
    // Step 2: Handle shape change
    if (shape_changed && config_.enable_dynamic_shapes) {
        auto status = handle_shape_change(inputs);
        if (status != core::Status::SUCCESS) {
            MI_LOG_ERROR("[Engine] Failed to handle shape change");
            return {};
        }
        
        last_input_shapes_ = current_shapes;
    }
    
    // Step 3: Execute inference
    return execute_inference(inputs);
}
```

**ä»»åŠ¡æ¸…å•**:
- [ ] æ‰©å±• `EngineConfig`
- [ ] ä¿®æ”¹ `Engine::build()`
- [ ] ä¿®æ”¹ `Engine::forward()`
- [ ] å®ç° `handle_shape_change()`
- [ ] é›†æˆæµ‹è¯•

**é¢„è®¡æ—¶é—´**: 5-7 å¤©

---

### Phase 3: ç®—å­æ”¯æŒå¢å¼º (1 å‘¨)

#### 3.1 ç¡®ä¿æ‰€æœ‰ç®—å­æ”¯æŒåŠ¨æ€å½¢çŠ¶

æ£€æŸ¥å¹¶æ›´æ–°æ¯ä¸ªç®—å­çš„ `infer_shape()` å®ç°ï¼š

```cpp
// ç¤ºä¾‹: Conv2D
core::Status Conv2D::infer_shape(
    const std::vector<core::Shape>& input_shapes,
    std::vector<core::Shape>& output_shapes
) const {
    // Validate inputs
    if (input_shapes.size() < 2) {
        return core::Status::ERROR_INVALID_ARGUMENT;
    }
    
    const auto& input_shape = input_shapes[0];  // [N, C_in, H, W]
    const auto& weight_shape = input_shapes[1]; // [C_out, C_in, K_h, K_w]
    
    // Support dynamic dimensions
    if (input_shape.ndim() != 4 || weight_shape.ndim() != 4) {
        return core::Status::ERROR_INVALID_ARGUMENT;
    }
    
    // Check channel consistency (skip if dynamic)
    if (input_shape[1] > 0 && weight_shape[1] > 0) {
        if (input_shape[1] != weight_shape[1]) {
            return core::Status::ERROR_INVALID_ARGUMENT;
        }
    }
    
    // Calculate output shape
    int64_t N = input_shape[0];  // May be -1 (dynamic)
    int64_t C_out = weight_shape[0];
    
    // Calculate spatial dimensions (handle dynamic H/W)
    int64_t H_out = -1;
    int64_t W_out = -1;
    
    if (input_shape[2] > 0 && input_shape[3] > 0) {
        H_out = (input_shape[2] + 2 * param_.padding_h - param_.kernel_h) / param_.stride_h + 1;
        W_out = (input_shape[3] + 2 * param_.padding_w - param_.kernel_w) / param_.stride_w + 1;
    }
    
    output_shapes = {core::Shape({N, C_out, H_out, W_out})};
    return core::Status::SUCCESS;
}
```

**ä»»åŠ¡æ¸…å•**:
- [ ] å®¡æŸ¥æ‰€æœ‰ç®—å­çš„ `infer_shape()`
- [ ] æ›´æ–°ä»¥æ”¯æŒåŠ¨æ€ç»´åº¦ï¼ˆ-1ï¼‰
- [ ] æ·»åŠ è¯¦ç»†çš„å½¢çŠ¶éªŒè¯
- [ ] å•å…ƒæµ‹è¯•æ¯ä¸ªç®—å­

**é¢„è®¡æ—¶é—´**: 4-5 å¤©

---

### Phase 4: æµ‹è¯•ä¸ä¼˜åŒ– (1-2 å‘¨)

#### 4.1 å•å…ƒæµ‹è¯•

```cpp
// tests/test_dynamic_shape_advanced.cpp

TEST(DynamicShapeTest, OptimizationProfile) {
    auto profile = std::make_shared<OptimizationProfile>();
    
    profile->set_shape_range(
        "input",
        Shape({1, 3, 224, 224}),   // min
        Shape({4, 3, 384, 384}),   // opt
        Shape({8, 3, 512, 512})    // max
    );
    
    // Valid shapes
    EXPECT_TRUE(profile->is_valid_for({{"input", Shape({1, 3, 224, 224})}}));
    EXPECT_TRUE(profile->is_valid_for({{"input", Shape({4, 3, 384, 384})}}));
    EXPECT_TRUE(profile->is_valid_for({{"input", Shape({8, 3, 512, 512})}}));
    
    // Invalid shapes
    EXPECT_FALSE(profile->is_valid_for({{"input", Shape({16, 3, 224, 224})}}));  // batch too large
    EXPECT_FALSE(profile->is_valid_for({{"input", Shape({4, 3, 1024, 1024})}})); // H/W too large
}

TEST(DynamicShapeTest, RuntimeShapeInference) {
    auto graph = create_test_graph();
    ShapeInferenceEngine engine;
    
    std::map<std::string, Shape> input_shapes1 = {
        {"input", Shape({1, 3, 224, 224})}
    };
    std::map<std::string, Shape> output_shapes1;
    
    auto status = engine.infer_shapes(graph, input_shapes1, output_shapes1);
    EXPECT_EQ(status, Status::SUCCESS);
    EXPECT_GT(output_shapes1.size(), 0);
    
    // Different input shape
    std::map<std::string, Shape> input_shapes2 = {
        {"input", Shape({4, 3, 384, 384})}
    };
    std::map<std::string, Shape> output_shapes2;
    
    status = engine.infer_shapes(graph, input_shapes2, output_shapes2);
    EXPECT_EQ(status, Status::SUCCESS);
    
    // Check cache
    auto stats = engine.get_cache_stats();
    EXPECT_EQ(stats.cache_misses, 2);
    EXPECT_EQ(stats.cache_hits, 0);
    
    // Reuse first shape (cache hit)
    std::map<std::string, Shape> output_shapes3;
    status = engine.infer_shapes(graph, input_shapes1, output_shapes3);
    
    stats = engine.get_cache_stats();
    EXPECT_EQ(stats.cache_hits, 1);
}

TEST(DynamicShapeTest, DynamicMemoryAllocation) {
    DynamicMemoryManager manager;
    
    // Prepare with profile
    OptimizationProfile profile;
    profile.set_shape_range("input", 
        Shape({1, 3, 224, 224}),
        Shape({4, 3, 384, 384}),
        Shape({8, 3, 512, 512})
    );
    
    MemoryPlan plan;  // From build time
    manager.prepare(profile, plan);
    
    // Allocate for actual shapes
    std::map<std::string, Shape> shapes1 = {
        {"input", Shape({1, 3, 224, 224})},
        {"conv1", Shape({1, 64, 112, 112})}
    };
    
    std::map<std::string, std::shared_ptr<void>> allocations1;
    auto status = manager.allocate_for_shapes(shapes1, allocations1);
    EXPECT_EQ(status, Status::SUCCESS);
    
    // Different shapes
    std::map<std::string, Shape> shapes2 = {
        {"input", Shape({4, 3, 384, 384})},
        {"conv1", Shape({4, 64, 192, 192})}
    };
    
    std::map<std::string, std::shared_ptr<void>> allocations2;
    status = manager.allocate_for_shapes(shapes2, allocations2);
    EXPECT_EQ(status, Status::SUCCESS);
    
    // Check memory reuse
    auto stats = manager.get_stats();
    EXPECT_GT(stats.pool_capacity, 0);
    EXPECT_LE(stats.reallocations, 1);  // Should reuse pool
}
```

#### 4.2 é›†æˆæµ‹è¯•

```cpp
// examples/dynamic_shape_advanced_demo.cpp

int main() {
    // 1. Load ONNX model with dynamic shapes
    OnnxParser parser;
    auto graph = parser.parse_from_file("resnet50_dynamic.onnx");
    
    // 2. Create optimization profile
    auto profile = std::make_shared<OptimizationProfile>();
    profile->set_shape_range(
        "input",
        Shape({1, 3, 224, 224}),   // min: single image
        Shape({4, 3, 384, 384}),   // opt: small batch, medium res
        Shape({16, 3, 512, 512})   // max: large batch, high res
    );
    
    // 3. Configure engine
    EngineConfig config;
    config.enable_dynamic_shapes = true;
    config.enable_shape_cache = true;
    config.enable_dynamic_memory = true;
    config.enable_profiling = true;
    config.optimization_profiles.push_back(profile);
    
    // 4. Build engine (uses optimal shapes)
    Engine engine(config);
    engine.build(graph);
    
    // 5. Run inference with different shapes
    std::vector<std::tuple<int, int, int>> test_cases = {
        {1, 224, 224},
        {2, 256, 256},
        {4, 384, 384},
        {8, 512, 512},
        {1, 224, 224},  // Repeat: should hit cache
    };
    
    for (const auto& [batch, height, width] : test_cases) {
        auto input = std::make_shared<Tensor>(
            Shape({batch, 3, height, width}),
            DataType::FLOAT32
        );
        
        // Fill with random data
        fill_random(input);
        
        MI_LOG_INFO("Testing shape: [" + std::to_string(batch) + ", 3, " +
                   std::to_string(height) + ", " + std::to_string(width) + "]");
        
        auto outputs = engine.forward({{"input", input}});
        
        // Print output shapes
        for (const auto& [name, tensor] : outputs) {
            MI_LOG_INFO("  Output: " + name + " " + tensor->shape().to_string());
        }
    }
    
    // 6. Print statistics
    auto shape_stats = engine.get_shape_inference_stats();
    MI_LOG_INFO("Shape inference cache hit rate: " + 
               std::to_string(shape_stats.hit_rate() * 100) + "%");
    
    auto memory_stats = engine.get_memory_stats();
    MI_LOG_INFO("Memory utilization: " + 
               std::to_string(memory_stats.utilization() * 100) + "%");
    MI_LOG_INFO("Memory reallocations: " + 
               std::to_string(memory_stats.reallocations));
    
    return 0;
}
```

#### 4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

```cpp
// tests/benchmark_dynamic_shape.cpp

void benchmark_dynamic_vs_static() {
    // Setup
    auto graph = load_test_model();
    
    // Test 1: Static shape (baseline)
    {
        EngineConfig config;
        config.enable_dynamic_shapes = false;
        Engine engine(config);
        engine.build(graph);
        
        auto input = create_input(Shape({4, 3, 224, 224}));
        
        auto start = std::chrono::high_resolution_clock::now();
        for (int i = 0; i < 1000; ++i) {
            engine.forward({{"input", input}});
        }
        auto end = std::chrono::high_resolution_clock::now();
        
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        std::cout << "Static shape: " << duration.count() << " ms\n";
    }
    
    // Test 2: Dynamic shape (same shape every time)
    {
        EngineConfig config;
        config.enable_dynamic_shapes = true;
        Engine engine(config);
        
        auto profile = std::make_shared<OptimizationProfile>();
        profile->set_shape_range("input",
            Shape({1, 3, 224, 224}),
            Shape({4, 3, 224, 224}),
            Shape({8, 3, 224, 224})
        );
        engine.add_optimization_profile(profile);
        engine.build(graph);
        
        auto input = create_input(Shape({4, 3, 224, 224}));
        
        auto start = std::chrono::high_resolution_clock::now();
        for (int i = 0; i < 1000; ++i) {
            engine.forward({{"input", input}});
        }
        auto end = std::chrono::high_resolution_clock::now();
        
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        std::cout << "Dynamic shape (cached): " << duration.count() << " ms\n";
        
        auto stats = engine.get_shape_inference_stats();
        std::cout << "Cache hit rate: " << stats.hit_rate() * 100 << "%\n";
    }
    
    // Test 3: Dynamic shape (varying shapes)
    {
        EngineConfig config;
        config.enable_dynamic_shapes = true;
        Engine engine(config);
        
        auto profile = std::make_shared<OptimizationProfile>();
        profile->set_shape_range("input",
            Shape({1, 3, 224, 224}),
            Shape({4, 3, 224, 224}),
            Shape({8, 3, 224, 224})
        );
        engine.add_optimization_profile(profile);
        engine.build(graph);
        
        std::vector<int> batches = {1, 2, 4, 8};
        
        auto start = std::chrono::high_resolution_clock::now();
        for (int i = 0; i < 1000; ++i) {
            int batch = batches[i % batches.size()];
            auto input = create_input(Shape({batch, 3, 224, 224}));
            engine.forward({{"input", input}});
        }
        auto end = std::chrono::high_resolution_clock::now();
        
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        std::cout << "Dynamic shape (varying): " << duration.count() << " ms\n";
        
        auto stats = engine.get_shape_inference_stats();
        std::cout << "Cache hit rate: " << stats.hit_rate() * 100 << "%\n";
    }
}
```

**ä»»åŠ¡æ¸…å•**:
- [ ] å•å…ƒæµ‹è¯•ï¼ˆæ‰€æœ‰æ–°ç»„ä»¶ï¼‰
- [ ] é›†æˆæµ‹è¯•ï¼ˆç«¯åˆ°ç«¯æµç¨‹ï¼‰
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å†…å­˜æ³„æ¼æ£€æµ‹
- [ ] å‹åŠ›æµ‹è¯•

**é¢„è®¡æ—¶é—´**: 7-10 å¤©

---

### Phase 5: æ–‡æ¡£ä¸ç¤ºä¾‹ (3-4 å¤©)

#### 5.1 ç”¨æˆ·æ–‡æ¡£

- [ ] `docs/DYNAMIC_SHAPE_TENSORRT_LEVEL.md` - å®Œæ•´ç”¨æˆ·æŒ‡å—
- [ ] `docs/OPTIMIZATION_PROFILE_GUIDE.md` - Profile ä½¿ç”¨æŒ‡å—
- [ ] API æ–‡æ¡£æ›´æ–°
- [ ] è¿ç§»æŒ‡å—ï¼ˆä»é™æ€åˆ°åŠ¨æ€ï¼‰

#### 5.2 ç¤ºä¾‹ä»£ç 

- [ ] `examples/dynamic_shape_basic.cpp` - åŸºç¡€ç”¨æ³•
- [ ] `examples/dynamic_shape_advanced.cpp` - é«˜çº§ç‰¹æ€§
- [ ] `examples/optimization_profile_demo.cpp` - Profile é…ç½®
- [ ] `examples/dynamic_batch_inference.cpp` - åŠ¨æ€ batch æ¨ç†

---

## ğŸ“… æ€»ä½“æ—¶é—´è¡¨

| Phase | å†…å®¹ | é¢„è®¡æ—¶é—´ | ä¾èµ– |
|-------|------|---------|------|
| Phase 1 | æ ¸å¿ƒåŸºç¡€è®¾æ–½ | 2-3 å‘¨ | - |
| Phase 2 | Engine é›†æˆ | 1-2 å‘¨ | Phase 1 |
| Phase 3 | ç®—å­æ”¯æŒ | 1 å‘¨ | Phase 1 |
| Phase 4 | æµ‹è¯•ä¸ä¼˜åŒ– | 1-2 å‘¨ | Phase 1-3 |
| Phase 5 | æ–‡æ¡£ä¸ç¤ºä¾‹ | 3-4 å¤© | Phase 1-4 |
| **æ€»è®¡** | | **5-7 å‘¨** | |

---

## ğŸ¯ é‡Œç¨‹ç¢‘

### Milestone 1: åŸºç¡€è®¾æ–½å®Œæˆ (Week 3)
- âœ… OptimizationProfile å®ç°
- âœ… ShapeInferenceEngine å®ç°
- âœ… DynamicMemoryManager å®ç°
- âœ… å•å…ƒæµ‹è¯•é€šè¿‡

### Milestone 2: é›†æˆå®Œæˆ (Week 5)
- âœ… Engine é›†æˆåŠ¨æ€ shape æ”¯æŒ
- âœ… æ‰€æœ‰ç®—å­æ”¯æŒåŠ¨æ€ç»´åº¦
- âœ… é›†æˆæµ‹è¯•é€šè¿‡

### Milestone 3: æ€§èƒ½è¾¾æ ‡ (Week 6)
- âœ… æ€§èƒ½å¼€é”€ < 5%ï¼ˆç›¸æ¯”é™æ€ï¼‰
- âœ… ç¼“å­˜å‘½ä¸­ç‡ > 80%
- âœ… å†…å­˜åˆ©ç”¨ç‡ > 70%

### Milestone 4: å‘å¸ƒå°±ç»ª (Week 7)
- âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡
- âœ… æ–‡æ¡£å®Œå–„
- âœ… ç¤ºä¾‹é½å…¨

---

## ğŸš€ æˆåŠŸæ ‡å‡†

å®Œæˆåï¼ŒMini-Infer åº”è¯¥èƒ½å¤Ÿï¼š

1. **æ”¯æŒä»»æ„ç»´åº¦åŠ¨æ€**
   ```cpp
   Shape({-1, 3, -1, -1})  // âœ… å…¨åŠ¨æ€
   ```

2. **Optimization Profile**
   ```cpp
   profile->set_shape_range("input",
       Shape({1, 3, 224, 224}),
       Shape({4, 3, 384, 384}),
       Shape({8, 3, 512, 512})
   );
   ```

3. **è¿è¡Œæ—¶è‡ªåŠ¨é‡æ¨æ–­**
   ```cpp
   engine.forward(input_224);  // Auto infer
   engine.forward(input_384);  // Auto infer
   engine.forward(input_224);  // Cache hit!
   ```

4. **é«˜æ•ˆå†…å­˜ç®¡ç†**
   - å†…å­˜æ± å¤ç”¨
   - æœ€å°åŒ–é‡åˆ†é…
   - < 5% å¼€é”€

5. **è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯**
   ```cpp
   auto stats = engine.get_shape_inference_stats();
   // cache_hits, cache_misses, hit_rate
   
   auto mem_stats = engine.get_memory_stats();
   // pool_capacity, pool_used, reallocations
   ```

---

## ğŸ“Š é¢„æœŸæ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | è¯´æ˜ |
|-----|------|------|
| Shape æ¨æ–­å¼€é”€ | < 1ms | ç¼“å­˜å‘½ä¸­æ—¶ |
| é¦–æ¬¡æ¨æ–­å¼€é”€ | < 10ms | å¤æ‚æ¨¡å‹ |
| ç¼“å­˜å‘½ä¸­ç‡ | > 80% | å…¸å‹åº”ç”¨ |
| å†…å­˜åˆ©ç”¨ç‡ | > 70% | é¿å…æµªè´¹ |
| é‡åˆ†é…æ¬¡æ•° | < 5% | å…¸å‹åº”ç”¨ä¸­ |
| API å¼€é”€ | < 5% | vs é™æ€ shape |

---

## ğŸ”„ ä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§

**å‘åå…¼å®¹**ï¼š
- é»˜è®¤ `enable_dynamic_shapes = false`
- ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹
- é€æ­¥è¿ç§»

**è¿ç§»è·¯å¾„**ï¼š
```cpp
// Old (static)
Engine engine(config);
engine.build(graph);

// New (dynamic, compatible)
EngineConfig config;
config.enable_dynamic_shapes = true;
config.optimization_profiles.push_back(profile);
Engine engine(config);
engine.build(graph);
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### TensorRT æ–‡æ¡£
- [Dynamic Shapes](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work_dynamic_shapes)
- [Optimization Profiles](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimization_profiles)
- [IExecutionContext](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_execution_context.html)

### å…¶ä»–æ¡†æ¶
- ONNX Runtime: Dynamic Shape Support
- PyTorch JIT: Dynamic Shapes
- TVM: Dynamic Shape Inference

---

## âœ… æ€»ç»“

è¿™ä¸ªè®¡åˆ’å°† Mini-Infer çš„åŠ¨æ€ Shape æ”¯æŒæå‡åˆ° **TensorRT çº§åˆ«**ï¼š

**æ ¸å¿ƒç‰¹æ€§**:
- âœ… Optimization Profileï¼ˆMin/Opt/Maxï¼‰
- âœ… è¿è¡Œæ—¶å½¢çŠ¶é‡æ¨æ–­
- âœ… å½¢çŠ¶ç¼“å­˜ä¼˜åŒ–
- âœ… åŠ¨æ€å†…å­˜ç®¡ç†
- âœ… ä»»æ„ç»´åº¦åŠ¨æ€

**å®æ–½å‘¨æœŸ**: 5-7 å‘¨

**èµ„æºéœ€æ±‚**: 1-2 åå¼€å‘è€…

**é£é™©è¯„ä¼°**: ä¸­ç­‰
- ä¸»è¦é£é™©ï¼šæ€§èƒ½ä¼˜åŒ–
- ç¼“è§£æªæ–½ï¼šå……åˆ†çš„æ€§èƒ½æµ‹è¯•å’Œprofiling

**é¢„æœŸæ”¶ç›Š**:
- ğŸš€ æ”¯æŒæ›´çµæ´»çš„æ¨ç†åœºæ™¯
- ğŸ“Š æ›´é«˜çš„å†…å­˜åˆ©ç”¨ç‡
- ğŸ¯ è¾¾åˆ°å·¥ä¸šçº§åŠ¨æ€ shape æ”¯æŒæ°´å¹³

è®©æˆ‘ä»¬å¼€å§‹å®æ–½å§ï¼ğŸš€


