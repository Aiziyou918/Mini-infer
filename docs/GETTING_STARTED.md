# Mini-Infer å¿«é€Ÿå…¥é—¨

æœ¬æ–‡æ¡£å°†å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ Mini-Infer æ¨ç†æ¡†æ¶ã€‚

## ç¬¬ä¸€æ­¥ï¼šæ„å»ºé¡¹ç›®

### Windows

```powershell
.\build.ps1 -Test
```

### Linux/macOS

```bash
chmod +x build.sh
./build.sh --test
```

## ç¬¬äºŒæ­¥ï¼šè¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

### åˆ›å»ºå¼ é‡

åˆ›å»ºæ–‡ä»¶ `my_first_app.cpp`ï¼š

```cpp
#include "mini_infer/mini_infer.h"
#include <iostream>

using namespace mini_infer;

int main() {
    // 1. åˆ›å»ºä¸€ä¸ªå¼ é‡
    core::Shape shape({1, 3, 224, 224});  // NCHW æ ¼å¼
    auto tensor = core::Tensor::create(shape, core::DataType::FLOAT32);
    
    std::cout << "åˆ›å»ºäº†å½¢çŠ¶ä¸º " << tensor->shape().to_string() 
              << " çš„å¼ é‡" << std::endl;
    std::cout << "æ€»å…ƒç´ æ•°: " << tensor->shape().numel() << std::endl;
    std::cout << "å†…å­˜å¤§å°: " << tensor->size_in_bytes() << " bytes" << std::endl;
    
    // 2. è®¿é—®å’Œå¡«å……æ•°æ®
    float* data = static_cast<float*>(tensor->data());
    for (int64_t i = 0; i < 10; ++i) {
        data[i] = static_cast<float>(i) * 0.1f;
        std::cout << "data[" << i << "] = " << data[i] << std::endl;
    }
    
    return 0;
}
```

ç¼–è¯‘è¿è¡Œï¼š

```bash
# Linux/macOS
g++ my_first_app.cpp -o my_first_app \
    -Iinclude \
    -Lbuild/lib \
    -lmini_infer_core \
    -lmini_infer_utils \
    -lpthread \
    -std=c++17

./my_first_app

# Windows (ä½¿ç”¨ MSVC)
cl my_first_app.cpp /std:c++17 /Iinclude /link build\lib\Release\mini_infer_core.lib
```

## ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨åç«¯

```cpp
#include "mini_infer/mini_infer.h"
#include <iostream>

using namespace mini_infer;

int main() {
    // è·å– CPU åç«¯
    auto backend = backends::BackendFactory::get_default_backend();
    
    std::cout << "ä½¿ç”¨åç«¯: " << backend->name() << std::endl;
    
    // åˆ†é…å†…å­˜
    size_t size = 1024 * sizeof(float);
    void* ptr = backend->allocate(size);
    
    // åˆå§‹åŒ–ä¸º 0
    backend->memset(ptr, 0, size);
    
    // å¡«å……æ•°æ®
    float* data = static_cast<float*>(ptr);
    for (int i = 0; i < 10; ++i) {
        data[i] = i * 0.5f;
    }
    
    // é‡Šæ”¾å†…å­˜
    backend->deallocate(ptr);
    
    std::cout << "å†…å­˜æ“ä½œå®Œæˆ" << std::endl;
    
    return 0;
}
```

## ç¬¬å››æ­¥ï¼šæ„å»ºè®¡ç®—å›¾

```cpp
#include "mini_infer/mini_infer.h"
#include <iostream>

using namespace mini_infer;

int main() {
    // åˆ›å»ºè®¡ç®—å›¾
    auto graph = std::make_shared<graph::Graph>();
    
    // æ·»åŠ èŠ‚ç‚¹
    auto input = graph->create_node("input");
    auto conv1 = graph->create_node("conv1");
    auto relu1 = graph->create_node("relu1");
    auto pool1 = graph->create_node("pool1");
    auto output = graph->create_node("output");
    
    // è¿æ¥èŠ‚ç‚¹
    graph->connect("input", "conv1");
    graph->connect("conv1", "relu1");
    graph->connect("relu1", "pool1");
    graph->connect("pool1", "output");
    
    // è®¾ç½®è¾“å…¥è¾“å‡º
    graph->set_inputs({"input"});
    graph->set_outputs({"output"});
    
    // éªŒè¯å›¾
    auto status = graph->validate();
    if (status == core::Status::SUCCESS) {
        std::cout << "âœ“ å›¾éªŒè¯é€šè¿‡" << std::endl;
    }
    
    // æ‹“æ‰‘æ’åº
    std::vector<std::shared_ptr<graph::Node>> sorted_nodes;
    status = graph->topological_sort(sorted_nodes);
    
    if (status == core::Status::SUCCESS) {
        std::cout << "æ‰§è¡Œé¡ºåº: ";
        for (const auto& node : sorted_nodes) {
            std::cout << node->name() << " -> ";
        }
        std::cout << "å®Œæˆ" << std::endl;
    }
    
    return 0;
}
```

è¾“å‡ºï¼š
```
âœ“ å›¾éªŒè¯é€šè¿‡
æ‰§è¡Œé¡ºåº: input -> conv1 -> relu1 -> pool1 -> output -> å®Œæˆ
```

## ç¬¬äº”æ­¥ï¼šä½¿ç”¨æ¨ç†å¼•æ“

```cpp
#include "mini_infer/mini_infer.h"
#include <iostream>

using namespace mini_infer;

int main() {
    // 1. æ„å»ºè®¡ç®—å›¾
    auto graph = std::make_shared<graph::Graph>();
    // ... æ·»åŠ èŠ‚ç‚¹å’Œè¿æ¥ ...
    
    // 2. é…ç½®å¼•æ“
    runtime::EngineConfig config;
    config.device_type = core::DeviceType::CPU;
    config.enable_profiling = true;
    
    // 3. åˆ›å»ºå¼•æ“
    runtime::Engine engine(config);
    
    // 4. æ„å»ºå¼•æ“ï¼ˆè¿™ä¼šä¼˜åŒ–å›¾å¹¶åˆ†é…å†…å­˜ï¼‰
    auto status = engine.build(graph);
    if (status != core::Status::SUCCESS) {
        std::cerr << "å¼•æ“æ„å»ºå¤±è´¥" << std::endl;
        return 1;
    }
    
    // 5. å‡†å¤‡è¾“å…¥æ•°æ®
    core::Shape input_shape({1, 3, 224, 224});
    auto input_tensor = core::Tensor::create(input_shape, core::DataType::FLOAT32);
    
    // å¡«å……è¾“å…¥æ•°æ®
    float* input_data = static_cast<float*>(input_tensor->data());
    for (int64_t i = 0; i < input_tensor->shape().numel(); ++i) {
        input_data[i] = 0.5f;  // ç¤ºä¾‹æ•°æ®
    }
    
    // 6. æ‰§è¡Œæ¨ç†
    std::unordered_map<std::string, std::shared_ptr<core::Tensor>> inputs;
    inputs["input"] = input_tensor;
    
    std::unordered_map<std::string, std::shared_ptr<core::Tensor>> outputs;
    status = engine.forward(inputs, outputs);
    
    if (status == core::Status::SUCCESS) {
        std::cout << "âœ“ æ¨ç†æˆåŠŸ" << std::endl;
        
        // è·å–è¾“å‡º
        auto output_tensor = outputs["output"];
        std::cout << "è¾“å‡ºå½¢çŠ¶: " << output_tensor->shape().to_string() << std::endl;
    }
    
    // 7. æŸ¥çœ‹æ€§èƒ½ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨äº† profilingï¼‰
    if (config.enable_profiling) {
        std::cout << engine.get_profiling_info() << std::endl;
    }
    
    return 0;
}
```

## å¸¸ç”¨ API é€ŸæŸ¥

### å¼ é‡æ“ä½œ

```cpp
// åˆ›å»ºå¼ é‡
auto tensor = core::Tensor::create({N, C, H, W}, core::DataType::FLOAT32);

// è®¿é—®æ•°æ®
float* data = static_cast<float*>(tensor->data());

// è·å–ä¿¡æ¯
tensor->shape()           // å½¢çŠ¶
tensor->dtype()           // æ•°æ®ç±»å‹
tensor->size_in_bytes()   // å­—èŠ‚å¤§å°
tensor->empty()           // æ˜¯å¦ä¸ºç©º

// é‡å¡‘
tensor->reshape(new_shape);
```

### å›¾æ“ä½œ

```cpp
// åˆ›å»ºå›¾å’ŒèŠ‚ç‚¹
auto graph = std::make_shared<graph::Graph>();
auto node = graph->create_node("node_name");

// è¿æ¥èŠ‚ç‚¹
graph->connect("src_node", "dst_node");

// è®¾ç½®è¾“å…¥è¾“å‡º
graph->set_inputs({"input1", "input2"});
graph->set_outputs({"output"});

// éªŒè¯å’Œæ’åº
graph->validate();
graph->topological_sort(sorted_nodes);
```

### å¼•æ“æ“ä½œ

```cpp
// åˆ›å»ºå¼•æ“
runtime::EngineConfig config;
config.device_type = core::DeviceType::CPU;
runtime::Engine engine(config);

// æ„å»ºå’Œæ‰§è¡Œ
engine.build(graph);
engine.forward(inputs, outputs);

// è·å–ä¿¡æ¯
engine.get_input_names();
engine.get_output_names();
```

### æ—¥å¿—

```cpp
// è®¾ç½®æ—¥å¿—çº§åˆ«
utils::Logger::get_instance().set_level(utils::LogLevel::INFO);

// ä½¿ç”¨æ—¥å¿—
MI_LOG_DEBUG("è°ƒè¯•ä¿¡æ¯");
MI_LOG_INFO("æ™®é€šä¿¡æ¯");
MI_LOG_WARNING("è­¦å‘Šä¿¡æ¯");
MI_LOG_ERROR("é”™è¯¯ä¿¡æ¯");
```

## ä¸‹ä¸€æ­¥

- é˜…è¯» [API æ–‡æ¡£](API.md) äº†è§£è¯¦ç»†æ¥å£
- é˜…è¯» [æ¶æ„æ–‡æ¡£](ARCHITECTURE.md) äº†è§£è®¾è®¡åŸç†
- æŸ¥çœ‹ `examples/` ç›®å½•ä¸‹çš„æ›´å¤šç¤ºä¾‹
- å°è¯•å®ç°è‡ªå·±çš„ç®—å­

## å¸¸è§é—®é¢˜

**Q: å¦‚ä½•åœ¨è‡ªå·±çš„ CMake é¡¹ç›®ä¸­ä½¿ç”¨ Mini-Inferï¼Ÿ**

A: åœ¨ CMakeLists.txt ä¸­æ·»åŠ ï¼š

```cmake
add_subdirectory(path/to/Mini-Infer)
target_link_libraries(your_target PRIVATE mini_infer_runtime)
```

**Q: æ”¯æŒå“ªäº›æ•°æ®ç±»å‹ï¼Ÿ**

A: ç›®å‰æ”¯æŒ FLOAT32, FLOAT16, INT32, INT8, UINT8, BOOLã€‚

**Q: å¦‚ä½•å¯ç”¨ GPU æ”¯æŒï¼Ÿ**

A: GPU (CUDA) æ”¯æŒæ­£åœ¨å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚

**Q: æ€§èƒ½å¦‚ä½•ä¼˜åŒ–ï¼Ÿ**

A: 
- ä½¿ç”¨ Release æ¨¡å¼æ„å»º
- å¯ç”¨ç¼–è¯‘å™¨ä¼˜åŒ–é€‰é¡¹
- ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆå¦‚ FP16ï¼‰
- æœªæ¥ç‰ˆæœ¬å°†æ”¯æŒå›¾ä¼˜åŒ–å’Œç®—å­èåˆ

## è·å–å¸®åŠ©

- æŸ¥çœ‹ [Issues](https://github.com/your-repo/Mini-Infer/issues)
- é˜…è¯»[è´¡çŒ®æŒ‡å—](../CONTRIBUTING.md)
- æŸ¥çœ‹ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸš€

