# Shape æ¨æ–­å¿«é€Ÿå‚è€ƒ

## ä¸€è¡Œä»£ç å¯ç”¨

```cpp
Engine engine(config);
engine.build(graph);  // â† è‡ªåŠ¨å®Œæˆ Shape æ¨æ–­
```

å°±è¿™ä¹ˆç®€å•ï¼Engine åœ¨ build æ—¶è‡ªåŠ¨æ¨æ–­æ‰€æœ‰å½¢çŠ¶ã€‚

---

## å¸¸è§åœºæ™¯

### åœºæ™¯ 1: ONNX æ¨¡å‹

```cpp
// âœ… æœ€ç®€å• - å½¢çŠ¶ä¿¡æ¯å·²åœ¨ ONNX ä¸­
auto graph = parser.parse_from_file("model.onnx");
engine.build(graph);  // è‡ªåŠ¨æ¨æ–­
```

### åœºæ™¯ 2: æ‰‹åŠ¨æ„å»ºå›¾

```cpp
// åªéœ€è®¾ç½®è¾“å…¥å’Œæƒé‡çš„å½¢çŠ¶
auto input_tensor = std::make_shared<Tensor>();
input_tensor->reshape(Shape({1, 3, 224, 224}));  // â† è®¾ç½®è¾“å…¥
input_node->set_output_tensors({input_tensor});

auto weight = std::make_shared<Tensor>(
    Shape({64, 3, 7, 7}),  // â† è®¾ç½®æƒé‡
    DataType::FLOAT32
);
conv_node->set_input_tensors({nullptr, weight, bias});

// ä¸­é—´å±‚çš„å½¢çŠ¶ä¼šè‡ªåŠ¨æ¨æ–­
engine.build(graph);
```

### åœºæ™¯ 3: æŸ¥çœ‹æ¨æ–­æ—¥å¿—

```cpp
config.enable_profiling = true;  // â† å¯ç”¨è¯¦ç»†æ—¥å¿—
engine.build(graph);

// è¾“å‡º:
// [Engine] Node conv1 output[0] shape: [1, 64, 112, 112]
// [Engine] Node pool1 output[0] shape: [1, 64, 56, 56]
```

### åœºæ™¯ 4: æ£€æŸ¥ç‰¹å®šèŠ‚ç‚¹å½¢çŠ¶

```cpp
auto node = graph->get_node("conv1");
auto shape = node->output_tensors()[0]->shape();
std::cout << shape.to_string() << std::endl;  // [1, 64, 112, 112]
```

---

## å½¢çŠ¶æ¨æ–­é¡ºåº

```
ONNX è§£æ
  â†“ (è§£æè¾“å…¥/æƒé‡å½¢çŠ¶)
Engine::build()
  â†“ (æŒ‰æ‹“æ‰‘åº)
Node 1 â†’ infer_shape() â†’ [1, 64, 112, 112]
  â†“
Node 2 â†’ infer_shape() â†’ [1, 64, 56, 56]
  â†“
Node 3 â†’ infer_shape() â†’ [1, 1000]
```

---

## å¸¸è§é”™è¯¯

### âŒ é”™è¯¯ 1: å¯¹ç©º Tensor ä½¿ç”¨ reshape

```cpp
// âŒ é”™è¯¯ï¼šreshape() ä¸èƒ½ç”¨äºç©º tensor
auto tensor = std::make_shared<Tensor>();
tensor->reshape(Shape({1, 3, 224, 224}));  // å¤±è´¥ï¼shape ä»ä¸ºç©º

// âœ… æ­£ç¡®ï¼šåˆ›å»ºæ—¶æŒ‡å®šå½¢çŠ¶
auto tensor = std::make_shared<Tensor>(
    Shape({1, 3, 224, 224}),
    DataType::FLOAT32
);
```

**åŸå› **: `reshape()` è¦æ±‚æ–°æ—§å½¢çŠ¶çš„å…ƒç´ æ•°é‡ç›¸åŒï¼Œç©º tensor çš„ numel=0ï¼Œæ‰€ä»¥æ— æ³• reshapeã€‚

### âŒ é”™è¯¯ 2: æƒé‡æ²¡æœ‰å½¢çŠ¶

```cpp
// âŒ é”™è¯¯
auto weight = std::make_shared<Tensor>();  // å½¢çŠ¶ä¸ºç©º

// âœ… æ­£ç¡®
auto weight = std::make_shared<Tensor>(
    Shape({64, 3, 7, 7}),
    DataType::FLOAT32
);
```

### âŒ é”™è¯¯ 3: è¾“å…¥èŠ‚ç‚¹æ²¡æœ‰å½¢çŠ¶

```cpp
// âŒ é”™è¯¯
auto input_node = graph->create_node("input");
// æ²¡æœ‰è®¾ç½® output_tensors

// âœ… æ­£ç¡®
auto input_tensor = std::make_shared<Tensor>(
    Shape({1, 3, 224, 224}),
    DataType::FLOAT32
);
input_node->set_output_tensors({input_tensor});
```

### âŒ é”™è¯¯ 4: å½¢çŠ¶ä¸åŒ¹é…

```cpp
// âŒ é”™è¯¯
// Conv weight: [64, 32, 3, 3]  éœ€è¦ C_in=32
// ä½†è¾“å…¥æ˜¯:  [1, 16, 224, 224]  å®é™… C_in=16

// æ¨æ–­æ—¶ä¼šæŠ¥é”™: ERROR_INVALID_ARGUMENT
```

---

## ç®—å­å½¢çŠ¶è§„åˆ™

| ç®—å­ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ | å…¬å¼ |
|-----|---------|---------|-----|
| **Conv2D** | [N,C_in,H,W] | [N,C_out,H',W'] | H' = (H+2P-K)/S+1 |
| **Pooling** | [N,C,H,W] | [N,C,H',W'] | H' = (H+2P-K)/S+1 |
| **Linear** | [...,in_f] | [...,out_f] | ä¿æŒå‰é¢ç»´åº¦ |
| **ReLU** | [ä»»æ„] | [ç›¸åŒ] | ä¸æ”¹å˜å½¢çŠ¶ |
| **Flatten** | [N,C,H,W] | [N,C*H*W] | ä» axis=1 å±•å¹³ |
| **Reshape** | [...] | [...] | æ€»å…ƒç´ æ•°ç›¸åŒ |

---

## åŠ¨æ€å½¢çŠ¶

### å½“å‰æ”¯æŒæƒ…å†µ

âœ… **æ”¯æŒ**ï¼š
- åŠ¨æ€ batch sizeï¼ˆç¬¬ 0 ç»´ï¼‰
- ONNX åŠ¨æ€ç»´åº¦è‡ªåŠ¨è¯†åˆ«
- Forward æ—¶å…è®¸ä¸åŒ batch

âš ï¸ **é™åˆ¶**ï¼š
- åªæ”¯æŒåŠ¨æ€ batchï¼Œå…¶ä»–ç»´åº¦å¿…é¡»å›ºå®š
- å†…å­˜è§„åˆ’åŸºäºé»˜è®¤ batch=1
- æ— è¿è¡Œæ—¶é‡æ¨æ–­

è¯¦è§ï¼š[åŠ¨æ€ Shape æ”¯æŒæ–‡æ¡£](DYNAMIC_SHAPE_SUPPORT.md)

### ä½¿ç”¨æ–¹æ³•

```cpp
// 1. ONNX ä¸­å®šä¹‰åŠ¨æ€ç»´åº¦
// input shape = [-1, 3, 224, 224]

// 2. Build æ—¶ä½¿ç”¨é»˜è®¤ batch=1
engine.build(graph);  // å†…éƒ¨ä½¿ç”¨ [1, 3, 224, 224]

// 3. Forward æ—¶å¯ä»¥ä½¿ç”¨ä¸åŒ batch
auto input_batch1 = std::make_shared<Tensor>(
    Shape({1, 3, 224, 224}),  // âœ… batch=1
    DataType::FLOAT32
);
engine.forward({{"input", input_batch1}});

auto input_batch8 = std::make_shared<Tensor>(
    Shape({8, 3, 224, 224}),  // âœ… batch=8ï¼ˆå…è®¸ï¼‰
    DataType::FLOAT32
);
engine.forward({{"input", input_batch8}});
```

### æ³¨æ„äº‹é¡¹

```cpp
// âœ… å…è®¸ï¼šå˜åŒ– batch size
Shape({1, 3, 224, 224});  // build
Shape({8, 3, 224, 224});  // forward - OK

// âŒ ä¸å…è®¸ï¼šå˜åŒ–å…¶ä»–ç»´åº¦
Shape({1, 3, 224, 224});  // build
Shape({1, 3, 256, 256});  // forward - ERROR!
```

---

## è°ƒè¯•æŠ€å·§

### 1. æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹å½¢çŠ¶

```cpp
for (const auto& [name, node] : graph->nodes()) {
    if (!node->output_tensors().empty()) {
        auto shape = node->output_tensors()[0]->shape();
        std::cout << name << ": " << shape.to_string() << std::endl;
    }
}
```

### 2. æ£€æŸ¥å½¢çŠ¶æ¨æ–­å¤±è´¥çš„èŠ‚ç‚¹

```cpp
config.enable_profiling = true;
engine.build(graph);

// æŸ¥çœ‹æ—¥å¿—ä¸­çš„ WARNING:
// [Engine] Failed to infer shape for node: conv1
```

### 3. éªŒè¯è¾“å…¥å½¢çŠ¶

```cpp
// åœ¨ forward å‰æ£€æŸ¥
auto expected = graph->get_node("input")->output_tensors()[0]->shape();
auto actual = input_tensor->shape();
if (expected.to_string() != actual.to_string()) {
    // å½¢çŠ¶ä¸åŒ¹é…
}
```

---

## å®Œæ•´ç¤ºä¾‹

```cpp
#include "mini_infer/importers/onnx_parser.h"
#include "mini_infer/runtime/engine.h"

int main() {
    // 1. åŠ è½½ ONNX æ¨¡å‹
    mini_infer::importers::OnnxParser parser;
    auto graph = parser.parse_from_file("model.onnx");
    
    // 2. é…ç½® Engine
    mini_infer::runtime::EngineConfig config;
    config.enable_graph_optimization = true;
    config.enable_memory_planning = true;
    config.enable_profiling = true;  // â† æŸ¥çœ‹å½¢çŠ¶æ¨æ–­è¿‡ç¨‹
    
    // 3. æ„å»º Engineï¼ˆè‡ªåŠ¨æ¨æ–­å½¢çŠ¶ï¼‰
    mini_infer::runtime::Engine engine(config);
    engine.build(graph);  // â† Shape æ¨æ–­åœ¨è¿™é‡Œå‘ç”Ÿ
    
    // 4. æŸ¥çœ‹å†…å­˜è§„åˆ’ç»“æœï¼ˆåŸºäºå‡†ç¡®çš„å½¢çŠ¶ï¼‰
    const auto& plan = engine.get_memory_plan();
    std::cout << "Original memory: " 
              << plan.original_memory / 1024.0 << " KB\n";
    std::cout << "Optimized memory: " 
              << plan.total_memory / 1024.0 << " KB\n";
    std::cout << "Memory saving: " 
              << plan.memory_saving_ratio * 100.0f << "%\n";
    
    // 5. è¿è¡Œæ¨ç†
    auto input = std::make_shared<mini_infer::core::Tensor>(
        mini_infer::core::Shape({1, 3, 224, 224}),
        mini_infer::core::DataType::FLOAT32
    );
    
    auto outputs = engine.forward({{"input", input}});
    
    return 0;
}
```

---

## æ›´å¤šä¿¡æ¯

- **è¯¦ç»†æ–‡æ¡£**: `docs/SHAPE_INFERENCE.md`
- **å®ç°ç»†èŠ‚**: `docs/SHAPE_INFERENCE_IMPLEMENTATION.md`
- **ç¤ºä¾‹ç¨‹åº**: `examples/shape_inference_demo.cpp`
- **å•å…ƒæµ‹è¯•**: `tests/test_shape_inference.cpp`

---

## è®°ä½è¿™äº›

âœ… **è‡ªåŠ¨å®Œæˆ** - Engine build æ—¶è‡ªåŠ¨æ¨æ–­æ‰€æœ‰å½¢çŠ¶  
âœ… **ONNX å‹å¥½** - è‡ªåŠ¨è§£æ ONNX æ¨¡å‹çš„å½¢çŠ¶ä¿¡æ¯  
âœ… **é”™è¯¯æå‰** - build é˜¶æ®µæ£€æµ‹å½¢çŠ¶é”™è¯¯ï¼Œä¸æ˜¯ forward æ—¶  
âœ… **è¯¦ç»†æ—¥å¿—** - enable_profiling æŸ¥çœ‹æ¯ä¸ªèŠ‚ç‚¹çš„å½¢çŠ¶  
âœ… **å†…å­˜å‡†ç¡®** - å½¢çŠ¶æ­£ç¡®åï¼Œå†…å­˜ç»Ÿè®¡æ‰å‡†ç¡®  

å°±è¿™ä¹ˆç®€å•ï¼ğŸ‰

