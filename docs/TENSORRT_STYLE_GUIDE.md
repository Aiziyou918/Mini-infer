# TensorRTé£æ ¼Kernelæ¶æ„æŒ‡å—

## ğŸ¯ æ¶æ„æ¦‚è§ˆ

Mini-Inferç°åœ¨é‡‡ç”¨TensorRTé£æ ¼çš„Kernelç®¡ç†æ¶æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç®—å­å±‚ (Operators)              â”‚
â”‚  Conv2D, Linear, etc.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kernelæ¥å£å±‚                    â”‚
â”‚  GEMMKernel, Im2ColKernel       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Registryç³»ç»Ÿ (è‡ªåŠ¨dispatch)     â”‚
â”‚  - GEMMRegistry_NN              â”‚
â”‚  - GEMMRegistry_NT              â”‚
â”‚  - Im2ColRegistry               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  CPU  â”‚      â”‚   CUDA     â”‚
â”‚ Impl  â”‚      â”‚   Impl     â”‚
â”‚(è‡ªæ³¨å†Œ)â”‚      â”‚  (è‡ªæ³¨å†Œ)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### 1. **è‡ªåŠ¨æ³¨å†Œï¼ˆAuto-Registrationï¼‰**

ç±»ä¼¼TensorRTçš„IPluginRegistryï¼Œå®ç°åœ¨ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨æ³¨å†Œï¼š

```cpp
// src/kernels/cpu/gemm_cpu.cpp

namespace cpu {
    // å®ç°
    template<typename T>
    void gemm_nn_impl(const T* A, const T* B, T* C, int M, int N, int K) {
        // CPUå®ç°
    }
    
    // å¯ç”¨æ€§æ£€æŸ¥
    bool is_cpu_available() {
        return true;
    }
}

// è‡ªåŠ¨æ³¨å†Œï¼ˆç¨‹åºå¯åŠ¨æ—¶æ‰§è¡Œï¼‰
static auto register_gemm_nn_float = AutoRegister<
    GEMMRegistry_NN<float>,      // æ³¨å†Œè¡¨ç±»å‹
    GEMMFunc_NN<float>            // å‡½æ•°ç±»å‹
>(
    KernelBackend::CPU,           // Backendç±»å‹
    cpu::gemm_nn_impl<float>,     // å‡½æ•°æŒ‡é’ˆ
    cpu::is_cpu_available,        // å¯ç”¨æ€§æ£€æŸ¥å™¨
    100                           // ä¼˜å…ˆçº§ï¼ˆè¶Šé«˜è¶Šä¼˜å…ˆï¼‰
);
```

### 2. **Registry Dispatch**

è¿è¡Œæ—¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°ï¼š

```cpp
template<typename T>
void GEMMKernel::gemm_nn(..., KernelBackend backend) {
    // ä»Registryè·å–kernelå‡½æ•°
    auto func = GEMMRegistry_NN<T>::instance().get_best_kernel();
    
    // æ‰§è¡Œ
    func(A, B, C, M, N, K);
}
```

### 3. **é›¶è™šå‡½æ•°å¼€é”€**

ä½¿ç”¨å‡½æ•°æŒ‡é’ˆä»£æ›¿è™šå‡½æ•°ï¼š
- âœ… æ— vtableæŸ¥æ‰¾
- âœ… æ”¯æŒinlineä¼˜åŒ–
- âœ… å…¼å®¹CUDA kernel

## ğŸ“ æ·»åŠ æ–°Backendå®ç°

### ç¤ºä¾‹ï¼šæ·»åŠ AVX2ä¼˜åŒ–ç‰ˆæœ¬

#### Step 1: åˆ›å»ºå®ç°æ–‡ä»¶

```cpp
// src/kernels/cpu/gemm_cpu_avx2.cpp

#include "mini_infer/kernels/gemm.h"
#include <immintrin.h>  // AVX2 intrinsics

namespace mini_infer {
namespace kernels {
namespace cpu {
namespace avx2 {

// AVX2ä¼˜åŒ–å®ç°
template<typename T>
void gemm_nn_impl(const T* A, const T* B, T* C, int M, int N, int K) {
    // AVX2å‘é‡åŒ–å®ç°
    for (int m = 0; m < M; ++m) {
        for (int n = 0; n < N; n += 8) {  // Process 8 floats at once
            __m256 sum = _mm256_setzero_ps();
            
            for (int k = 0; k < K; ++k) {
                __m256 a = _mm256_broadcast_ss(&A[m * K + k]);
                __m256 b = _mm256_load_ps(&B[k * N + n]);
                sum = _mm256_fmadd_ps(a, b, sum);
            }
            
            _mm256_store_ps(&C[m * N + n], sum);
        }
    }
}

// æ£€æŸ¥AVX2æ”¯æŒ
bool is_avx2_available() {
    #ifdef __AVX2__
        return true;
    #else
        // Runtime detection
        __builtin_cpu_init();
        return __builtin_cpu_supports("avx2");
    #endif
}

} // namespace avx2
} // namespace cpu

// è‡ªåŠ¨æ³¨å†ŒAVX2ç‰ˆæœ¬
static auto register_gemm_nn_float_avx2 = AutoRegister<
    GEMMRegistry_NN<float>,
    GEMMFunc_NN<float>
>(
    KernelBackend::CPU_AVX2,
    cpu::avx2::gemm_nn_impl<float>,
    cpu::avx2::is_avx2_available,
    200  // æ›´é«˜ä¼˜å…ˆçº§ï¼ˆä¼˜å…ˆä½¿ç”¨AVX2ï¼‰
);

} // namespace kernels
} // namespace mini_infer
```

#### Step 2: æ›´æ–°CMakeLists.txt

```cmake
# src/kernels/CMakeLists.txt

set(KERNEL_SOURCES
    cpu/gemm_cpu.cpp
    cpu/gemm_cpu_avx2.cpp  # æ–°å¢
    cpu/im2col_cpu.cpp
)

# AVX2ç¼–è¯‘é€‰é¡¹
if(MSVC)
    set_source_files_properties(cpu/gemm_cpu_avx2.cpp 
        PROPERTIES COMPILE_FLAGS "/arch:AVX2")
else()
    set_source_files_properties(cpu/gemm_cpu_avx2.cpp 
        PROPERTIES COMPILE_FLAGS "-mavx2 -mfma")
endif()
```

#### Step 3: è‡ªåŠ¨ç”Ÿæ•ˆ

æ— éœ€ä¿®æ”¹ä»»ä½•å…¶ä»–ä»£ç ï¼Registryä¼šè‡ªåŠ¨ï¼š
1. å¯åŠ¨æ—¶æ³¨å†ŒAVX2ç‰ˆæœ¬
2. æ£€æµ‹CPUæ˜¯å¦æ”¯æŒAVX2
3. å¦‚æœæ”¯æŒï¼Œä¼˜å…ˆä½¿ç”¨AVX2ç‰ˆæœ¬

## ğŸš€ æ·»åŠ CUDA Backend

### ç¤ºä¾‹ï¼šCUDAå®ç°

```cpp
// src/kernels/cuda/gemm_cuda.cu

#include "mini_infer/kernels/gemm.h"

namespace mini_infer {
namespace kernels {
namespace cuda {

// CUDA kernel
template<typename T>
__global__ void gemm_nn_kernel(const T* A, const T* B, T* C, 
                               int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        T sum = 0;
        for (int k = 0; k < K; ++k) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

// Host wrapper
template<typename T>
void gemm_nn_impl(const T* A, const T* B, T* C, int M, int N, int K) {
    dim3 block(16, 16);
    dim3 grid((N + 15) / 16, (M + 15) / 16);
    
    gemm_nn_kernel<<<grid, block>>>(A, B, C, M, N, K);
    cudaDeviceSynchronize();
}

// CUDAå¯ç”¨æ€§æ£€æŸ¥
bool is_cuda_available() {
    int device_count = 0;
    cudaGetDeviceCount(&device_count);
    return device_count > 0;
}

} // namespace cuda

// è‡ªåŠ¨æ³¨å†ŒCUDAç‰ˆæœ¬
static auto register_gemm_nn_float_cuda = AutoRegister<
    GEMMRegistry_NN<float>,
    GEMMFunc_NN<float>
>(
    KernelBackend::CUDA,
    cuda::gemm_nn_impl<float>,
    cuda::is_cuda_available,
    300  // æœ€é«˜ä¼˜å…ˆçº§ï¼ˆä¼˜å…ˆä½¿ç”¨CUDAï¼‰
);

} // namespace kernels
} // namespace mini_infer
```

## ğŸ“Š ä¼˜å…ˆçº§ç³»ç»Ÿ

RegistryæŒ‰ä¼˜å…ˆçº§é™åºé€‰æ‹©kernelï¼š

| Backend | ä¼˜å…ˆçº§ | è¯´æ˜ |
|---------|--------|------|
| CUDA_CUBLAS | 400 | cuBLASä¼˜åŒ– |
| CUDA | 300 | åŸºç¡€CUDA |
| CPU_BLAS | 250 | OpenBLAS/MKL |
| CPU_AVX512 | 220 | AVX512å‘é‡åŒ– |
| CPU_AVX2 | 200 | AVX2å‘é‡åŒ– |
| CPU | 100 | åŸºç¡€CPUå®ç° |

## ğŸ¯ ä¸TensorRTå¯¹æ¯”

### TensorRT Pluginç³»ç»Ÿ

```cpp
// TensorRTé£æ ¼
class MyPlugin : public IPluginV2 {
    int enqueue(...) override {
        // è°ƒç”¨kernel
        myKernel<<<>>>(...);
    }
};

// æ³¨å†ŒPlugin
REGISTER_TENSORRT_PLUGIN(MyPluginCreator);
```

### Mini-Infer Kernelç³»ç»Ÿ

```cpp
// Mini-Inferé£æ ¼
namespace cpu {
    void my_kernel(...) { /* å®ç° */ }
}

// è‡ªåŠ¨æ³¨å†Œ
static auto reg = AutoRegister<MyRegistry, MyFunc>(
    KernelBackend::CPU,
    cpu::my_kernel,
    []() { return true; },
    100
);
```

**å…±åŒç‚¹**ï¼š
- âœ… è‡ªåŠ¨æ³¨å†Œæœºåˆ¶
- âœ… è¿è¡Œæ—¶dispatch
- âœ… é›¶è™šå‡½æ•°å¼€é”€
- âœ… æ”¯æŒå¤šBackend

**å·®å¼‚**ï¼š
- TensorRT: Pluginæ˜¯ç®—å­å±‚ï¼Œä½¿ç”¨è™šå‡½æ•°
- Mini-Infer: Kernelæ˜¯è®¡ç®—å±‚ï¼Œä½¿ç”¨å‡½æ•°æŒ‡é’ˆ

## ğŸ”„ è¿ç§»æŒ‡å—

### ä»æ—§é£æ ¼è¿ç§»

**æ—§ä»£ç ï¼ˆæ‰‹åŠ¨dispatchï¼‰**ï¼š
```cpp
template<typename T>
void GEMMKernel::gemm_nn(..., KernelBackend backend) {
    switch(backend) {
        case CPU:
            cpu::gemm_nn_impl(...);
            break;
        case CUDA:
            cuda::gemm_nn_impl(...);
            break;
    }
}
```

**æ–°ä»£ç ï¼ˆè‡ªåŠ¨dispatchï¼‰**ï¼š
```cpp
template<typename T>
void GEMMKernel::gemm_nn(..., KernelBackend backend) {
    auto func = GEMMRegistry_NN<T>::instance().get_best_kernel();
    func(...);
}
```

### å‘åå…¼å®¹

å…¬å…±æ¥å£ä¿æŒä¸å˜ï¼š
```cpp
// ç”¨æˆ·ä»£ç æ— éœ€ä¿®æ”¹
GEMMKernel::gemm_nn<float>(A, B, C, M, N, K);
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [TensorRT Plugin Development](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#add_custom_layer)
- [TensorRT IPluginRegistry](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_plugin_registry.html)
- [PyTorch Dispatcher](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/dispatch/Dispatcher.h)

## âœ… æœ€ä½³å®è·µ

1. **ä¸€ä¸ªBackendä¸€ä¸ªæ–‡ä»¶** - æ˜“äºç®¡ç†å’Œç¼–è¯‘
2. **ä½¿ç”¨å‘½åç©ºé—´** - `cpu::`, `cuda::`, `avx2::` ç­‰
3. **æä¾›å¯ç”¨æ€§æ£€æŸ¥** - è¿è¡Œæ—¶æ£€æµ‹ç¡¬ä»¶æ”¯æŒ
4. **è®¾ç½®åˆç†ä¼˜å…ˆçº§** - ç¡®ä¿è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å®ç°
5. **ä¿æŒæ¥å£ç®€å•** - å‡½æ•°ç­¾åå°½é‡ç®€æ´
6. **æ·»åŠ æ–‡æ¡£æ³¨é‡Š** - è¯´æ˜å®ç°ç‰¹æ€§å’Œä¼˜åŒ–

## ğŸ‰ æ€»ç»“

TensorRTé£æ ¼çš„Kernelæ¶æ„å¸¦æ¥ï¼š
- âœ… é›¶å¼€é”€æŠ½è±¡
- âœ… è‡ªåŠ¨Backendé€‰æ‹©
- âœ… æ˜“äºæ‰©å±•
- âœ… ä»£ç è§£è€¦
- âœ… å·¥ä¸šçº§è®¾è®¡

å®Œç¾å¯¹é½TensorRTçš„è®¾è®¡ç†å¿µï¼ğŸš€
