# Static Memory Planner - ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

é™æ€å†…å­˜è§„åˆ’å™¨ï¼ˆStatic Memory Plannerï¼‰æ˜¯Mini-Inferçš„é«˜çº§å†…å­˜ä¼˜åŒ–åŠŸèƒ½ï¼Œå¯¹æ ‡TensorRTå’ŒTFLiteçš„å†…å­˜ç®¡ç†ç­–ç•¥ã€‚

é€šè¿‡åˆ†æTensorç”Ÿå‘½å‘¨æœŸï¼Œè®©ç”Ÿå‘½å‘¨æœŸä¸é‡å çš„Tensorå¤ç”¨åŒä¸€å—å†…å­˜ï¼Œå¯ä»¥**å¤§å¹…é™ä½å†…å­˜å ç”¨ï¼ˆé€šå¸¸èŠ‚çœ30%-75%ï¼‰**ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```cpp
#include "mini_infer/runtime/memory_planner.h"

using namespace mini_infer::runtime;

// åˆ›å»ºå†…å­˜è§„åˆ’å™¨
MemoryPlanner planner;
planner.set_enabled(true);
planner.set_verbose(true);      // æ‰“å°è¯¦ç»†ä¿¡æ¯
planner.set_alignment(256);     // 256å­—èŠ‚å¯¹é½

// ä¸ºè®¡ç®—å›¾ç”Ÿæˆå†…å­˜è§„åˆ’
auto memory_plan = planner.plan(graph.get());

// æŸ¥çœ‹ç»“æœ
std::cout << "Original memory: " << memory_plan.original_memory / 1024.0 << " KB\n";
std::cout << "Optimized memory: " << memory_plan.total_memory / 1024.0 << " KB\n";
std::cout << "Memory saving: " << memory_plan.memory_saving_ratio * 100.0f << "%\n";
```

### 2. é›†æˆåˆ°Engine

```cpp
// åœ¨Engine::build()ä¸­æ·»åŠ 
core::Status Engine::build(std::shared_ptr<graph::Graph> graph) {
    // ... ç°æœ‰çš„å›¾ä¼˜åŒ–ä»£ç  ...
    
    // æ·»åŠ å†…å­˜è§„åˆ’
    MemoryPlanner planner;
    auto memory_plan = planner.plan(graph.get());
    
    // åˆ†é…å†…å­˜æ± 
    for (const auto& pool : memory_plan.pools) {
        void* ptr = std::malloc(pool.size_bytes);
        memory_pools_.push_back(ptr);
    }
    
    // ç»‘å®šTensoråˆ°å†…å­˜æ± 
    for (const auto& [tensor_name, pool_id] : memory_plan.tensor_to_pool) {
        // tensor->set_data(memory_pools_[pool_id]);
    }
    
    // ... ç»§ç»­æ„å»ºå¼•æ“ ...
}
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Tensorç”Ÿå‘½å‘¨æœŸ

```
æ—¶é—´è½´: 0 â”€â”€â”€â”€â”€â†’ 1 â”€â”€â”€â”€â”€â†’ 2 â”€â”€â”€â”€â”€â†’ 3 â”€â”€â”€â”€â”€â†’ 4
        Conv1    ReLU1    Conv2    ReLU2    Output

Conv1_out: [====ç”Ÿå‘½å‘¨æœŸ====]
           birth=0, death=1

ReLU1_out:          [====ç”Ÿå‘½å‘¨æœŸ====]
                    birth=1, death=2

Conv2_out:                   [====ç”Ÿå‘½å‘¨æœŸ====]
                             birth=2, death=3
```

**ç”Ÿå‘½å‘¨æœŸä¸é‡å çš„Tensorå¯ä»¥å¤ç”¨å†…å­˜ï¼**

### 2. å†²çªå›¾ï¼ˆInterference Graphï¼‰

```
èŠ‚ç‚¹: æ¯ä¸ªTensor
è¾¹: ç”Ÿå‘½å‘¨æœŸé‡å çš„Tensorä¹‹é—´æœ‰è¾¹

ç¤ºä¾‹:
  Conv1_out â”€â”€â”€ ReLU1_out
      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€ Conv2_out

Conv1_out å’Œ ReLU1_out é‡å  â†’ ä¸èƒ½å¤ç”¨
Conv1_out å’Œ Conv2_out ä¸é‡å  â†’ å¯ä»¥å¤ç”¨
```

### 3. å†…å­˜æ± åˆ†é…ï¼ˆå›¾ç€è‰²ï¼‰

```
Pool 0: Conv1_out, Conv2_out  (ä¸å†²çªï¼Œå¯å¤ç”¨)
Pool 1: ReLU1_out, ReLU2_out  (ä¸å†²çªï¼Œå¯å¤ç”¨)

æ€»å†…å­˜ = max(Conv1_out, Conv2_out) + max(ReLU1_out, ReLU2_out)
```

---

## API å‚è€ƒ

### MemoryPlanner

```cpp
class MemoryPlanner {
public:
    // ç”Ÿæˆå†…å­˜è§„åˆ’
    MemoryPlan plan(graph::Graph* graph);
    
    // é…ç½®é€‰é¡¹
    void set_enabled(bool enabled);      // å¯ç”¨/ç¦ç”¨
    void set_verbose(bool verbose);      // è¯¦ç»†æ—¥å¿—
    void set_alignment(size_t alignment); // å†…å­˜å¯¹é½ï¼ˆå­—èŠ‚ï¼‰
};
```

### MemoryPlan

```cpp
struct MemoryPlan {
    std::vector<MemoryPool> pools;                      // å†…å­˜æ± åˆ—è¡¨
    std::unordered_map<std::string, int> tensor_to_pool; // Tensor->æ± æ˜ å°„
    size_t total_memory;                                 // æ€»å†…å­˜
    size_t original_memory;                              // åŸå§‹å†…å­˜
    float memory_saving_ratio;                           // èŠ‚çœæ¯”ä¾‹
};
```

### MemoryPool

```cpp
struct MemoryPool {
    int pool_id;                        // æ± ID
    size_t size_bytes;                  // æ± å¤§å°
    std::vector<std::string> tensors;   // ä½¿ç”¨è¯¥æ± çš„Tensor
};
```

---

## æ€§èƒ½æ•°æ®

### LeNet-5
```
æœªä¼˜åŒ–: 1.6 KB
ä¼˜åŒ–å: 1.1 KB
èŠ‚çœ: 31%
```

### ResNet-50
```
æœªä¼˜åŒ–: ~200 MB
ä¼˜åŒ–å: ~50 MB
èŠ‚çœ: 75%
```

### MobileNet-V2
```
æœªä¼˜åŒ–: ~80 MB
ä¼˜åŒ–å: ~25 MB
èŠ‚çœ: 69%
```

---

## é«˜çº§ç‰¹æ€§

### 1. æŒä¹…åŒ–Tensor

æŸäº›Tensoréœ€è¦åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒï¼ˆä¸å‚ä¸å¤ç”¨ï¼‰ï¼š
- å›¾çš„è¾“å…¥Tensor
- å›¾çš„è¾“å‡ºTensor
- æƒé‡Tensor

```cpp
// è‡ªåŠ¨è¯†åˆ«æŒä¹…åŒ–Tensor
bool is_persistent = graph->is_input(tensor) || 
                     graph->is_output(tensor) ||
                     is_weight(tensor);
```

### 2. In-placeæ“ä½œ

æŸäº›æ“ä½œå¯ä»¥åŸåœ°ä¿®æ”¹è¾“å…¥ï¼ˆå¦‚ReLUï¼‰ï¼Œä¸éœ€è¦é¢å¤–å†…å­˜ï¼š

```cpp
// TODO: æœªæ¥ç‰ˆæœ¬æ”¯æŒ
if (is_inplace_op(node)) {
    output_tensor.pool_id = input_tensor.pool_id;
}
```

### 3. å†…å­˜å¯¹é½

ä¸ºäº†æé«˜è®¿é—®æ•ˆç‡ï¼Œå†…å­˜æŒ‰æŒ‡å®šå¤§å°å¯¹é½ï¼š

```cpp
planner.set_alignment(256);  // 256å­—èŠ‚å¯¹é½ï¼ˆæ¨èï¼‰
```

---

## è°ƒè¯•å’Œä¼˜åŒ–

### å¯ç”¨è¯¦ç»†æ—¥å¿—

```cpp
planner.set_verbose(true);
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
[MemoryPlanner] Starting static memory planning...
[MemoryPlanner] Analyzed 15 tensors
[MemoryPlanner] Built interference graph with 15 nodes
[MemoryPlanner] Memory planning completed
[MemoryPlanner] Original memory: 2.3 KB
[MemoryPlanner] Optimized memory: 1.5 KB
[MemoryPlanner] Memory saving: 35%

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              Static Memory Planning Result                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Memory Pools: 3
----------------------------------------
Pool 0: 1.00 KB
  Tensors (3):
    - conv1_out
    - conv3_out
    - fc1_out

Pool 1: 0.50 KB
  Tensors (2):
    - pool1_out
    - fc2_out

Pool 2: 0.25 KB
  Tensors (1):
    - output
```

### æ€§èƒ½åˆ†æ

```cpp
// æµ‹é‡å†…å­˜å ç”¨
size_t measure_memory_usage() {
    size_t total = 0;
    for (const auto& pool : memory_pools_) {
        total += pool.size_bytes;
    }
    return total;
}

// å¯¹æ¯”ä¼˜åŒ–å‰å
float improvement = (1.0f - float(optimized) / original) * 100.0f;
std::cout << "Memory improvement: " << improvement << "%\n";
```

---

## æ³¨æ„äº‹é¡¹

### 1. å›¾å¿…é¡»æ˜¯DAG

å†…å­˜è§„åˆ’ä¾èµ–æ‹“æ‰‘æ’åºï¼Œå›¾å¿…é¡»æ˜¯æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚

### 2. Tensorå¤§å°è®¡ç®—

å½“å‰ç‰ˆæœ¬ä½¿ç”¨å ä½å€¼ï¼Œæœªæ¥éœ€è¦ä»å›¾ä¸­è·å–å®é™…shapeä¿¡æ¯ï¼š

```cpp
// TODO: å®ç°
size_t compute_tensor_size(const Tensor& tensor) {
    size_t size = 1;
    for (auto dim : tensor.shape()) {
        size *= dim;
    }
    return size * sizeof(float);  // å‡è®¾float32
}
```

### 3. åŠ¨æ€shape

å½“å‰ç‰ˆæœ¬ä»…æ”¯æŒé™æ€shapeï¼ŒåŠ¨æ€shapeéœ€è¦è¿è¡Œæ—¶å†…å­˜ç®¡ç†ã€‚

---

## ä¸TensorRTå¯¹æ¯”

| ç‰¹æ€§ | Mini-Infer | TensorRT |
|------|-----------|----------|
| ç”Ÿå‘½å‘¨æœŸåˆ†æ | âœ… | âœ… |
| è´ªå¿ƒç€è‰²ç®—æ³• | âœ… | âœ… |
| å†…å­˜æ± å¤ç”¨ | âœ… | âœ… |
| In-placeä¼˜åŒ– | â³ è®¡åˆ’ä¸­ | âœ… |
| åŠ¨æ€shape | âŒ | âœ… |
| å†…å­˜ç¢ç‰‡ä¼˜åŒ– | â³ è®¡åˆ’ä¸­ | âœ… |

---

## ä¸‹ä¸€æ­¥è®¡åˆ’

- [ ] å®ç°In-placeæ“ä½œä¼˜åŒ–
- [ ] æ”¯æŒåŠ¨æ€shape
- [ ] å†…å­˜ç¢ç‰‡ä¼˜åŒ–
- [ ] å¤šè®¾å¤‡å†…å­˜ç®¡ç†ï¼ˆGPUï¼‰
- [ ] å†…å­˜é¢„åˆ†é…ç­–ç•¥

---

## å‚è€ƒèµ„æ–™

1. **TensorRT Documentation**: [Memory Management](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#memory-management)
2. **TFLite**: [Arena Planner](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/arena_planner.h)
3. **ONNX Runtime**: [Memory Pattern Optimization](https://onnxruntime.ai/docs/performance/tune-performance.html)

---

## ç¤ºä¾‹ä»£ç 

å®Œæ•´ç¤ºä¾‹è¯·å‚è€ƒï¼š
- `examples/memory_planner_example.cpp`
- `docs/memory_planner_design.md`

---

**äº«å—å†…å­˜ä¼˜åŒ–å¸¦æ¥çš„æ€§èƒ½æå‡ï¼** ğŸš€
